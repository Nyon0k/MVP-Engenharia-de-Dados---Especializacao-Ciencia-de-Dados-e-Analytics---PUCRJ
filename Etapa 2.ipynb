{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57f13bb8-a8e2-4b61-a8c3-698c0fcd473c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df828e0c-f551-4469-9bee-fc672beaaa3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_base_pd_bronze = spark.table(\"puc.bronze.base_pd\")\n",
    "display(df_base_pd_bronze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b58eaf5-241d-427b-9236-d3539cf8a1cc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Normalização Colunas Categóricas"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Normalizador (Python)\n",
    "# -----------------------------\n",
    "def _normalize_text(s: str) -> str:\n",
    "    if s is None:\n",
    "        return None\n",
    "\n",
    "    s = s.strip().lower()\n",
    "\n",
    "    # remove acentos (NFKD)\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    s = \"\".join(ch for ch in s if not unicodedata.combining(ch))\n",
    "    \n",
    "    # remove hífen no início ou fim\n",
    "    s = re.sub(r\"(^-|-$)\", \" \", s)\n",
    "\n",
    "    # remove hífen que não está entre letras/números\n",
    "    s = re.sub(r\"(?<![a-z0-9])-|-(?![a-z0-9])\", \" \", s)\n",
    "\n",
    "    # remove hífens duplicados\n",
    "    s = re.sub(r\"-{2,}\", \" \", s)\n",
    "\n",
    "    # normaliza espaços\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "    # colapsa múltiplos espaços\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "    return s if s != \"\" else None\n",
    "\n",
    "normalize_udf = F.udf(_normalize_text, T.StringType())\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Função principal\n",
    "# -----------------------------\n",
    "def normalize_categorical_columns(\n",
    "    df,\n",
    "    max_distinct_per_col: int = 200_000,\n",
    "    keep_nulls: bool = True,\n",
    "    canonical_strategy: str = \"most_frequent\",  # \"most_frequent\" ou \"normalized\"\n",
    "):\n",
    "    \"\"\"\n",
    "    - Detecta colunas string.\n",
    "    - Para cada coluna: (valor_original -> chave_normalizada).\n",
    "    - Escolhe um valor canônico por chave_normalizada e aplica no DF.\n",
    "    \"\"\"\n",
    "\n",
    "    # Colunas categóricas (string)\n",
    "    str_cols = [f.name for f in df.schema.fields if isinstance(f.dataType, T.StringType)]\n",
    "\n",
    "    if not str_cols:\n",
    "        return df, {}\n",
    "\n",
    "    mapping_by_col = {}\n",
    "\n",
    "    df_out = df\n",
    "\n",
    "    for c in str_cols:\n",
    "        # valores distintos + contagem\n",
    "        stats = (\n",
    "            df.select(F.col(c).alias(\"orig\"))\n",
    "              .where(F.col(\"orig\").isNotNull() if keep_nulls else F.lit(True))\n",
    "              .groupBy(\"orig\")\n",
    "              .count()\n",
    "        )\n",
    "\n",
    "        # (opcional) proteção contra colunas com cardinalidade absurda\n",
    "        distinct_cnt = stats.count()\n",
    "        if distinct_cnt > max_distinct_per_col:\n",
    "            print(f\"[SKIP] Coluna '{c}' tem {distinct_cnt:,} distintos (limite {max_distinct_per_col:,}).\")\n",
    "            continue\n",
    "\n",
    "        stats = stats.withColumn(\"norm\", normalize_udf(F.col(\"orig\")))\n",
    "\n",
    "        # define o canônico por norm\n",
    "        # - most_frequent: usa o valor original mais frequente (bom quando você quer preservar formato humano)\n",
    "        # - normalized: usa a própria string normalizada (bom quando você quer padronização total)\n",
    "        if canonical_strategy == \"most_frequent\":\n",
    "            w = F.window  # só pra não confundir; não vamos usar window function aqui\n",
    "            canon = (\n",
    "                stats.where(F.col(\"norm\").isNotNull())\n",
    "                     .groupBy(\"norm\")\n",
    "                     .agg(\n",
    "                         F.max(F.struct(F.col(\"count\"), F.col(\"orig\"))).alias(\"mx\")  # max por count\n",
    "                     )\n",
    "                     .select(\n",
    "                         F.col(\"norm\"),\n",
    "                         F.col(\"mx.orig\").alias(\"canonical\")\n",
    "                     )\n",
    "            )\n",
    "        elif canonical_strategy == \"normalized\":\n",
    "            canon = (\n",
    "                stats.select(\"norm\")\n",
    "                     .where(F.col(\"norm\").isNotNull())\n",
    "                     .distinct()\n",
    "                     .withColumn(\"canonical\", F.col(\"norm\"))\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"canonical_strategy deve ser 'most_frequent' ou 'normalized'\")\n",
    "\n",
    "        # junta pra criar mapa orig -> canonical\n",
    "        map_df = (\n",
    "            stats.select(\"orig\", \"norm\")\n",
    "                 .join(canon, on=\"norm\", how=\"left\")\n",
    "                 .select(\"orig\", \"canonical\")\n",
    "                 .where(F.col(\"orig\").isNotNull())\n",
    "        )\n",
    "\n",
    "        # traz o mapeamento para o driver (ok se distinct não for gigantesco)\n",
    "        pairs = map_df.collect()\n",
    "        mapping = {row[\"orig\"]: row[\"canonical\"] for row in pairs if row[\"canonical\"] is not None}\n",
    "        mapping_by_col[c] = mapping\n",
    "\n",
    "        # aplica no DF via create_map (mais rápido do que UDF pra substituir)\n",
    "        # cria um map literal: { \"Avião\" -> \"aviao\", \"aviao\" -> \"aviao\", ... }\n",
    "        if mapping:\n",
    "            kv = []\n",
    "            for k, v in mapping.items():\n",
    "                kv.extend([F.lit(k), F.lit(v)])\n",
    "            m = F.create_map(*kv)\n",
    "\n",
    "            df_out = df_out.withColumn(\n",
    "                c,\n",
    "                F.coalesce(m[F.col(c)], F.col(c))\n",
    "            )\n",
    "\n",
    "        print(f\"[OK] Coluna '{c}': {distinct_cnt:,} distintos, {len(mapping):,} mapeados/normalizados.\")\n",
    "\n",
    "    return df_out, mapping_by_col\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Uso\n",
    "# -----------------------------\n",
    "# df = spark.table(\"silver.base_flat\")  # exemplo\n",
    "df_norm, mapping = normalize_categorical_columns(\n",
    "    df_base_pd_bronze,\n",
    "    max_distinct_per_col=200000,\n",
    "    canonical_strategy=\"normalized\"  # deixa tudo padronizado (Avião/aviao -> aviao)\n",
    ")\n",
    "\n",
    "# Ex.: ver o mapeamento gerado para uma coluna\n",
    "# mapping[\"setor\"]  # dict {original: canonical}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3288b714-51cd-4d90-aaed-095530aa050b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema_silver = StructType([\n",
    "    StructField(\"id_cliente\", StringType(), False),\n",
    "    StructField(\"data_solicitacao\", DateType(), False),\n",
    "    StructField(\"uf\", StringType(), True),\n",
    "    StructField(\"regiao\", StringType(), True),\n",
    "    StructField(\"idade\", IntegerType(), True),\n",
    "    StructField(\"tempo_conta_anos\", IntegerType(), True),\n",
    "    StructField(\"canal\", StringType(), True),\n",
    "    StructField(\"produto\", StringType(), True),\n",
    "    StructField(\"escolaridade\", StringType(), True),\n",
    "    StructField(\"estado_civil\", StringType(), True),\n",
    "    StructField(\"vinculo_emprego\", StringType(), True),\n",
    "    StructField(\"setor\", StringType(), True),\n",
    "    StructField(\"usa_internet_banking\", BooleanType(), True),\n",
    "    StructField(\"possui_cartao_credito\", BooleanType(), True),\n",
    "    StructField(\"possui_investimentos\", BooleanType(), True),\n",
    "    StructField(\"possui_seguro\", BooleanType(), True),\n",
    "    StructField(\"qtd_produtos_bancarios\", IntegerType(), True),\n",
    "    StructField(\"renda_mensal_atual\", DoubleType(), True),\n",
    "    StructField(\"renda_mensal_anterior\", DoubleType(), True),\n",
    "    StructField(\"atrasos_passados\", IntegerType(), True),\n",
    "    StructField(\"score_credito\", DoubleType(), True),\n",
    "    StructField(\"parcelas\", IntegerType(), True),\n",
    "    StructField(\"valor_emprestimo\", DoubleType(), True),\n",
    "    StructField(\"taxa_juros_mensal\", DoubleType(), True),\n",
    "    StructField(\"valor_parcela\", DoubleType(), True),\n",
    "    StructField(\"dti\", DoubleType(), True),\n",
    "    StructField(\"frequencia_transacoes\", IntegerType(), True),\n",
    "    StructField(\"valor_emprestimos_anteriores\", DoubleType(), True),\n",
    "    StructField(\"pd_true\", DoubleType(), True),\n",
    "    StructField(\"inadimplente\", IntegerType(), True),\n",
    "    StructField(\"ead\", DoubleType(), True),\n",
    "    StructField(\"lgd\", DoubleType(), True),\n",
    "    StructField(\"valor_recuperado\", DoubleType(), True),\n",
    "    StructField(\"pd_model\", DoubleType(), True),\n",
    "    StructField(\"tempo_emprego_anos\", DoubleType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d03ef6b6-8485-4483-817a-d4f8304029f9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Forçando tipos corretos das colunas"
    }
   },
   "outputs": [],
   "source": [
    "df_processado_1 = df_norm \\\n",
    "    .withColumn(\"data_solicitacao\", to_date(\"data_solicitacao\")) \\\n",
    "    .select([col(c).cast(schema_silver[c].dataType) for c in df_norm.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9215fe18-966d-4dd8-b384-ae2982b05025",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Higienização básica de colunas categóricas"
    }
   },
   "outputs": [],
   "source": [
    "str_cols = [c for c in df_processado_1.columns if isinstance(schema_silver[c].dataType, StringType)]\n",
    "\n",
    "for c in str_cols:\n",
    "    df_processado_1 = df_processado_1.withColumn(c, trim(regexp_replace(col(c), \"[^\\\\p{L}\\\\p{N}\\\\s_./@-]\", \"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "589bd0f1-31ac-4855-b682-87e3063d9ef7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_processado_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffe4bcac-1fef-4fc9-a569-a9d86215abb3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Coletando Domínios"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, LongType,\n",
    "    FloatType, DoubleType, DecimalType, DateType\n",
    ")\n",
    "from pyspark.sql.functions import col, min as spark_min, max as spark_max\n",
    "\n",
    "df = df_processado_1\n",
    "\n",
    "resultados = []\n",
    "\n",
    "for c in df.columns:\n",
    "    dtype = df.schema[c].dataType\n",
    "\n",
    "    # --- TRATAR id_cliente COMO NUMÉRICO ---\n",
    "    if c == \"id_cliente\":\n",
    "        df_num = df.withColumn(c, col(c).cast(\"long\"))\n",
    "        stats = df_num.agg(\n",
    "            spark_min(col(c)).alias(\"min\"),\n",
    "            spark_max(col(c)).alias(\"max\")\n",
    "        ).collect()[0]\n",
    "\n",
    "        resultados.append((c, \"intervalo_num_forcado\", str(stats[\"min\"]), str(stats[\"max\"])))\n",
    "        continue\n",
    "\n",
    "    # --- COLUNAS NUMÉRICAS ---\n",
    "    if isinstance(dtype, (IntegerType, LongType, FloatType, DoubleType, DecimalType)):\n",
    "        stats = df.agg(\n",
    "            spark_min(col(c)).alias(\"min\"),\n",
    "            spark_max(col(c)).alias(\"max\")\n",
    "        ).collect()[0]\n",
    "        resultados.append((c, \"intervalo_num\", str(stats[\"min\"]), str(stats[\"max\"])))\n",
    "\n",
    "    # --- COLUNAS DE DATA ---\n",
    "    elif isinstance(dtype, DateType):\n",
    "        stats = df.agg(\n",
    "            spark_min(col(c)).alias(\"min\"),\n",
    "            spark_max(col(c)).alias(\"max\")\n",
    "        ).collect()[0]\n",
    "        resultados.append((c, \"intervalo_data\", str(stats[\"min\"]), str(stats[\"max\"])))\n",
    "\n",
    "    # --- STRINGS / CATEGÓRICAS ---\n",
    "    elif isinstance(dtype, StringType):\n",
    "        valores = df.select(col(c)).distinct().limit(500).collect()\n",
    "        valores = [row[c] for row in valores]\n",
    "        resultados.append((c, \"categorico\", str(valores), None))\n",
    "\n",
    "    # --- QUALQUER OUTRO TIPO ---\n",
    "    else:\n",
    "        resultados.append((c, f\"tipo_{dtype}\", None, None))\n",
    "\n",
    "# Criar schema final\n",
    "schema = StructType([\n",
    "    StructField(\"coluna\", StringType(), True),\n",
    "    StructField(\"tipo\", StringType(), True),\n",
    "    StructField(\"valor_1\", StringType(), True),\n",
    "    StructField(\"valor_2\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_resultado = spark.createDataFrame(resultados, schema)\n",
    "display(df_resultado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7599053-8b1a-4a76-9189-550ed7350eb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# domínio fechado para UF (Brasil)\n",
    "UF_DOMAIN = [\n",
    "    \"ac\",\"al\",\"ap\",\"am\",\"ba\",\"ce\",\"df\",\"es\",\"go\",\"ma\",\"mt\",\"ms\",\"mg\",\n",
    "    \"pa\",\"pb\",\"pr\",\"pe\",\"pi\",\"rj\",\"rn\",\"rs\",\"ro\",\"rr\",\"sc\",\"sp\",\"se\",\"to\"\n",
    "]\n",
    "\n",
    "DOMAINS = {\n",
    "    # ----- CATEGÓRICOS (lista fechada) -----\n",
    "    \"uf\": {\"type\": \"set\", \"allowed\": UF_DOMAIN},\n",
    "    \"regiao\": {\"type\": \"set\", \"allowed\": [\"norte\",\"nordeste\",\"centro-oeste\",\"sudeste\",\"sul\"]},\n",
    "\n",
    "    # se você preferir fechar produto/canal com base no que existe hoje:\n",
    "    # (pode trocar por \"set\" com lista fixa depois)\n",
    "    \"canal\": {\"type\": \"set\", \"allowed\": ['app', 'internet banking', 'agencia', 'correspondente']},\n",
    "    \"produto\": {\n",
    "        \"type\": \"set\",\n",
    "        \"allowed\": ['consignado', 'veiculo', 'pessoal', 'home_equity', 'cartao'],\n",
    "        \"allow_null\": True\n",
    "    },\n",
    "    \"escolaridade\": {\"type\": \"set\", \"allowed\": ['medio', 'fundamental', 'pos', 'superior']},\n",
    "    \"estado_civil\": {\"type\": \"set\", \"allowed\": ['solteiro', 'divorciado', 'casado', 'viuvo']},\n",
    "    \"vinculo_emprego\": {\"type\": \"set\", \"allowed\": ['clt', 'autonomo', 'servidor', 'desempregado', 'empresario', 'estudante']},\n",
    "    \"setor\": {\"type\": \"set\", \"allowed\": ['educacao', 'saude', 'comercio', 'industria', 'ti', 'construcao', 'servicos', 'outro']},\n",
    "\n",
    "    # ----- BOOLEANOS / FLAGS -----\n",
    "    \"usa_internet_banking\": {\"type\": \"boolean\"},\n",
    "    \"possui_cartao_credito\": {\"type\": \"boolean\"},\n",
    "    \"possui_investimentos\": {\"type\": \"boolean\"},\n",
    "    \"possui_seguro\": {\"type\": \"boolean\"},\n",
    "    \"inadimplente\": {\"type\": \"set\", \"allowed\": [0, 1]},\n",
    "\n",
    "    # ----- DATAS -----\n",
    "    \"data_solicitacao\": {\"type\": \"date_range\", \"min\": \"1900-01-01\", \"max\": \"today\"},\n",
    "\n",
    "    # ----- NUMÉRICOS: intervalos (incluindo 0..+inf) -----\n",
    "    \"idade\": {\"type\": \"range\", \"min\": 0, \"max\": 120},\n",
    "    \"tempo_conta_anos\": {\"type\": \"range\", \"min\": 0, \"max\": None},          # 0..+inf\n",
    "    \"tempo_emprego_anos\": {\n",
    "        \"type\": \"range\", \"min\": 0, \"max\": None,        # 0..+inf\n",
    "        \"allow_null\": True\n",
    "    },\n",
    "\n",
    "    \"qtd_produtos_bancarios\": {\"type\": \"range_int\", \"min\": 0, \"max\": None},# inteiro 0..+inf\n",
    "    \"renda_mensal_atual\": {\n",
    "        \"type\": \"range\", \"min\": 0, \"max\": None,        # 0..+inf\n",
    "        \"allow_null\": True\n",
    "    },\n",
    "    \"renda_mensal_anterior\": {\n",
    "        \"type\": \"range\", \"min\": 0, \"max\": None,        # 0..+inf\n",
    "        \"allow_null\": True\n",
    "    },\n",
    "\n",
    "    \"atrasos_passados\": {\"type\": \"range_int\", \"min\": 0, \"max\": None},\n",
    "    \"score_credito\": {\"type\": \"range\", \"min\": 0, \"max\": 1000},\n",
    "\n",
    "    \"parcelas\": {\"type\": \"range_int\", \"min\": 1, \"max\": None},\n",
    "    \"valor_emprestimo\": {\"type\": \"range\", \"min\": 0, \"max\": None},\n",
    "    \"taxa_juros_mensal\": {\"type\": \"range\", \"min\": 0, \"max\": None},\n",
    "    \"valor_parcela\": {\"type\": \"range\", \"min\": 0, \"max\": None},\n",
    "    \"dti\": {\"type\": \"range\", \"min\": 0, \"max\": None},                       # normalmente >=0 (às vezes <=1 ou <=100)\n",
    "\n",
    "    \"frequencia_transacoes\": {\"type\": \"range\", \"min\": 0, \"max\": None},\n",
    "    \"valor_emprestimos_anteriores\": {\n",
    "        \"type\": \"range\", \"min\": 0, \"max\": None,\n",
    "        \"allow_null\": True\n",
    "    },\n",
    "\n",
    "    # Risco: probabilidades e parâmetros\n",
    "    \"pd_true\": {\"type\": \"range\", \"min\": 0, \"max\": 1},\n",
    "    \"pd_model\": {\"type\": \"range\", \"min\": 0, \"max\": 1},\n",
    "    \"lgd\": {\"type\": \"range\", \"min\": 0, \"max\": 1},\n",
    "\n",
    "    # Exposição e recuperações\n",
    "    \"ead\": {\"type\": \"range\", \"min\": 0, \"max\": None},\n",
    "    \"valor_recuperado\": {\"type\": \"range\", \"min\": 0, \"max\": None},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b92d6892-7ae2-4e12-be39-2612a71f5a87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "def _today_literal():\n",
    "    return F.current_date()\n",
    "\n",
    "def build_rule_expr(colname: str, rule: dict):\n",
    "    c = F.col(colname)\n",
    "\n",
    "    # se for nulo, você decide: considerar válido ou inválido\n",
    "    # aqui vou considerar NULO como inválido por padrão\n",
    "    not_null = c.isNotNull()\n",
    "\n",
    "    t = rule[\"type\"]\n",
    "\n",
    "    if t == \"set\":\n",
    "        allowed = rule[\"allowed\"]\n",
    "        allow_null = rule.get(\"allow_null\", False)\n",
    "\n",
    "        expr = c.isin(allowed)\n",
    "        if allow_null:\n",
    "            expr = expr | c.isNull()\n",
    "        else:\n",
    "            expr = expr & c.isNotNull()\n",
    "\n",
    "        return expr\n",
    "\n",
    "    if t == \"non_empty_string\":\n",
    "        return not_null & (F.length(F.trim(c)) > 0)\n",
    "\n",
    "    if t in (\"range\", \"range_int\"):\n",
    "        mn = rule.get(\"min\", None)\n",
    "        mx = rule.get(\"max\", None)\n",
    "        allow_null = rule.get(\"allow_null\", False)\n",
    "\n",
    "        # valida o intervalo (sem forçar not_null ainda)\n",
    "        expr = F.lit(True)\n",
    "        if mn is not None:\n",
    "            expr = expr & (c >= F.lit(mn))\n",
    "        if mx is not None:\n",
    "            expr = expr & (c <= F.lit(mx))\n",
    "\n",
    "        if t == \"range_int\":\n",
    "            expr = expr & (c == F.floor(c))\n",
    "\n",
    "        # trata NULL conforme allow_null\n",
    "        if allow_null:\n",
    "            expr = expr | c.isNull()\n",
    "        else:\n",
    "            expr = expr & c.isNotNull()\n",
    "\n",
    "        return expr\n",
    "\n",
    "    if t == \"date_range\":\n",
    "        mn = rule.get(\"min\", None)\n",
    "        mx = rule.get(\"max\", None)\n",
    "\n",
    "        expr = not_null\n",
    "        if mn is not None:\n",
    "            expr = expr & (c >= F.to_date(F.lit(mn)))\n",
    "        if mx == \"today\":\n",
    "            expr = expr & (c <= _today_literal())\n",
    "        elif mx is not None:\n",
    "            expr = expr & (c <= F.to_date(F.lit(mx)))\n",
    "        return expr\n",
    "    \n",
    "    if t == \"boolean\":\n",
    "        # aceita True/False; por padrão considera NULL como inválido\n",
    "        return c.isNotNull() & (c.isin(True, False))\n",
    "\n",
    "    raise ValueError(f\"Tipo de regra desconhecido: {t}\")\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "def validate_domains(df: DataFrame, domains: dict):\n",
    "    # só valida colunas que existem no DF\n",
    "    domains = {k: v for k, v in domains.items() if k in df.columns}\n",
    "\n",
    "    validations = {colname: build_rule_expr(colname, rule) for colname, rule in domains.items()}\n",
    "\n",
    "    # cria uma lista de arrays, cada um contendo o nome da coluna com erro ou array vazio\n",
    "    error_arrays = [\n",
    "        F.when(~ok_expr, F.array(F.lit(colname))).otherwise(F.array())\n",
    "        for colname, ok_expr in validations.items()\n",
    "    ]\n",
    "\n",
    "    # concatena todos os arrays em um único array de erros (garantido não-nulo)\n",
    "    erros = F.flatten(F.array(*error_arrays))  # sempre retorna array (pode ser vazio)\n",
    "\n",
    "    df_valid = (\n",
    "        df\n",
    "        .withColumn(\"__erros\", erros)\n",
    "        .withColumn(\"__qtd_erros\", F.size(F.col(\"__erros\")))\n",
    "        .withColumn(\"__is_valid\", F.col(\"__qtd_erros\") == 0)\n",
    "    )\n",
    "\n",
    "    # resumo por coluna (% inválido) - mantém sua lógica\n",
    "    total = df.count()\n",
    "    summary_rows = []\n",
    "    for colname, ok_expr in validations.items():\n",
    "        invalid = df.where(~ok_expr).count()\n",
    "        summary_rows.append((colname, invalid, total, invalid / total if total else 0.0))\n",
    "\n",
    "    summary = spark.createDataFrame(summary_rows, [\"coluna\", \"qtd_invalidos\", \"total\", \"pct_invalido\"])\n",
    "    return df_valid, summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fdd57ec-90d5-4c25-b26d-a56a2ff2c01c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_validado, resumo = validate_domains(df_processado_1, DOMAINS)\n",
    "\n",
    "# resumo (% inválido por coluna)\n",
    "display(resumo.orderBy(F.desc(\"pct_invalido\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eca0c421-485c-4c7b-93a1-7fa71d39509f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 2) separa em válidos e inválidos\n",
    "df_ok = df_validado.filter(F.col(\"__is_valid\") == True)\n",
    "df_bad = df_validado.filter(F.col(\"__is_valid\") == False)\n",
    "\n",
    "display(df_ok)\n",
    "\n",
    "# (opcional) remover colunas de auditoria dos válidos\n",
    "df_ok_clean = df_ok.drop(\"__erros\", \"__qtd_erros\", \"__is_valid\")\n",
    "\n",
    "# (recomendado) manter auditoria nos inválidos\n",
    "# df_bad mantém __erros, __qtd_erros, __is_valid\n",
    "\n",
    "# 3) salvar como tabelas Delta\n",
    "# ajuste os nomes/schema conforme seu projeto\n",
    "df_ok_clean.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"puc.silver.base_pd\")\n",
    "df_bad.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"puc.silver.base_pd_invalidos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdb72b99-14a9-4747-88c9-976b9f7983d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_base_pd_silver = spark.table(\"puc.silver.base_pd\")\n",
    "display(df_base_pd_silver)\n",
    "print(df_base_pd_silver.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d14afcd-fcca-4071-88a3-ab3d6774b1a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_base_pd_silver_invalidos = spark.table(\"puc.silver.base_pd_invalidos\")\n",
    "display(df_base_pd_silver_invalidos)\n",
    "print(df_base_pd_silver_invalidos.count())"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Etapa 2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
